{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from pytorch_lightning import LightningDataModule, Trainer\n",
    "\n",
    "from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class dqn(nn.Module):\n",
    "    def __init__(self, observation_dim, hidden_size, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(observation_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, observation):\n",
    "        return self.net(observation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_policy(\n",
    "    state, env, net, eplison=0.0, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    if np.random.random() < eplison:\n",
    "        action = env.action_space.sample()\n",
    "        return action\n",
    "    else:\n",
    "        state = torch.tensor([state]).to(device)\n",
    "        q_values = net(state)\n",
    "        _, action = torch.max(q_values, dim=1)\n",
    "        # action = int(action.item())\n",
    "        return q_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(torch.utils.data.dataset.IterableDataset):\n",
    "    def __init__(self, buffer, sample_size=200):\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for experience in self.buffer.sample(self.sample_size):\n",
    "            yield experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(name):\n",
    "    env = gym.make(name, render_mode=\"rgb_array\")\n",
    "    return env\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# env = create_env(\"LunarLander-v2\")\n",
    "# env.reset()\n",
    "# frame = env.render()\n",
    "# plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.nn.functional.smooth_l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class DeepQLearning(LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 env_name, \n",
    "                 policy=epsilon_policy, \n",
    "                 capacity=100_000, \n",
    "                 batch_size=256,\n",
    "                 lr=1e-3,\n",
    "                 hidden_size=128,\n",
    "                 gamma=0.99,\n",
    "                 loss_fn=torch.nn.functional.smooth_l1_loss,\n",
    "                 optimizer=torch.optim.AdamW,\n",
    "                 eps_start=1.0,\n",
    "                 eps_end=0.15,\n",
    "                 eps_last_episode=100,\n",
    "                 sample_per_epoch=10_000,\n",
    "                 sync_rate=10):\n",
    "        super().__init__()\n",
    "        self.env = create_env(env_name)\n",
    "        observation_size = self.env.observation_space.shape[0]\n",
    "        action_dim = self.env.action_space.n\n",
    "        self.q_net = dqn(hidden_size=hidden_size, observation_dim=observation_size, action_dim=action_dim)\n",
    "        self.target_q_net = copy.deepcopy(self.q_net)\n",
    "        self.policy = policy\n",
    "        self.buffer = ReplayBuffer(capacity=capacity)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        while len(self.buffer) <= self.hparams.sample_per_epoch:\n",
    "            print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
    "            self.play_episode(epsilon = self.hparams.eps_start)\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def play_episode(self, policy=None, epsilon=0.):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if policy:\n",
    "                action = policy(state, self.env, self.q_net, epsilon=epsilon)\n",
    "            else:\n",
    "                action = self.env.sample_action()\n",
    "            next_state, reward, done , _, _= self.env.step(action)\n",
    "            exp = (state, action, reward, done, next_state)\n",
    "            self.buffer.append(exp)\n",
    "            state = next_state\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.q_net(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        q_net_optimizer = self.hparams.optimizer(self.q_net.parameters(), lr=self.hparams.lr)\n",
    "        return [q_net_optimizer]\n",
    "    \n",
    "    def train_dataloaders(self):\n",
    "        dataset = RLDataset(self.buffer, self.hparams.sample_per_epoch)\n",
    "        dataloader = DataLoader(dataset=dataset,\n",
    "                                batch_size=self.hparams.batch_size)\n",
    "        return dataloader\n",
    "    \n",
    "    def training_step(self, batch, batch_inx):\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        actions = actions.unsqueeze(1)\n",
    "        rewards = rewards.unsqueeze(1)\n",
    "        dones = dones.unsqueeze(1)\n",
    "\n",
    "        state_action_values = self.q_net(states).gather(1, actions)\n",
    "        next_action_values, _ = self.target_q_net(next_states).max(dim=1, keepdim=True)\n",
    "        next_action_values[dones] = 0.0\n",
    "\n",
    "        expected_state_action_values = rewards + self.hparams.gamma * next_action_values\n",
    "\n",
    "        loss = self.hparams.loss_fn(state_action_values, expected_state_action_values)\n",
    "        self.log('episode/Q_Error', loss)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self):\n",
    "        epsilon = max(self.hparams.eps_end,\n",
    "                      self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episodes)\n",
    "        self.play_episode(policy=self.policy, epsilon=epsilon)\n",
    "        self.log('episodes/Return', self.env.return_queuel[-1])\n",
    "\n",
    "        if self.current_epoch % self.hparams.sync_rate == 0:\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r /content/lighting_logs/\n",
    "# !rm -r /contect/videos/\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir /content/lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "algo = DeepQLearning('LunarLander-v2')\n",
    "trainer = Trainer(qgus=num_gpus,\n",
    "                  max_epochs=10_000,\n",
    "                  callbacks=[EarlyStopping(monitor='episode/Return', mode=\"max\", patient=500)])\n",
    "Trainer.fit(algo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
